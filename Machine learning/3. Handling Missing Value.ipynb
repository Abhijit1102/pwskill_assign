{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4083bde4",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78887a",
   "metadata": {},
   "source": [
    "`Missing` values in a dataset refer to the absence of a particular value in a data point. It can occur due to various reasons, such as data entry errors, incomplete data, or data loss during the data collection process. Handling missing values is essential as it can affect the accuracy of the analysis and prediction of a machine learning model.\n",
    "\n",
    "Some of the reasons why handling missing values is essential are:\n",
    "\n",
    "1. Missing values can impact the statistical power of a dataset.\n",
    "2. Missing values can introduce bias in the data analysis.\n",
    "3. Missing values can impact the predictive accuracy of machine learning models.\n",
    "\n",
    "Some of the algorithms that are not affected by missing values are:\n",
    "\n",
    "1. Decision Trees: Decision trees can handle missing values in the data by using surrogate splits.\n",
    "2. andom Forests: Random forests can handle missing values by randomly selecting an imputed value for each missing value.\n",
    "3. Gradient Boosting Machines: Gradient boosting machines can handle missing values by using surrogate splits and imputing missing values based on the gradient of the loss function.\n",
    "4. aive Bayes: Naive Bayes assumes that the missing values are missing at random and can handle missing values by imputing the mode or median of the feature distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced6bcdb",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38a1ad",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in a dataset. Some of the commonly used techniques are:\n",
    "\n",
    "1. `Deleting rows with missing data`: This technique involves deleting the rows that have missing values. This method is useful when the number of missing values is small compared to the total number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b906ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A    B\n",
      "0  1  3.0\n",
      "1  2  4.0\n",
      "2  3  5.0\n",
      "4  5  7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [3, 4, 5, None, 7]})\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c9be1d",
   "metadata": {},
   "source": [
    "2. `Mean/median imputation`: This technique involves replacing missing values with the mean or median value of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5bab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A     B\n",
      "0  1  3.00\n",
      "1  2  4.00\n",
      "2  3  5.00\n",
      "3  4  4.75\n",
      "4  5  7.00\n"
     ]
    }
   ],
   "source": [
    "# create a sample dataframe\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [3, 4, 5, None, 7]})\n",
    "\n",
    "# calculate the mean of column B\n",
    "mean = df['B'].mean()\n",
    "\n",
    "# fill missing values with the mean value\n",
    "df = df.fillna(mean)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c420d5",
   "metadata": {},
   "source": [
    "3.`Interpolation`: This technique involves estimating the missing values based on the values of other observations. It is useful when the data is sequential or has some kind of pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d79a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A    B\n",
      "0  1  3.0\n",
      "1  2  4.0\n",
      "2  3  5.0\n",
      "3  4  6.0\n",
      "4  5  7.0\n"
     ]
    }
   ],
   "source": [
    "# create a sample dataframe\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [3, 4, None, 6, 7]})\n",
    "\n",
    "# interpolate missing values\n",
    "df['B'] = df['B'].interpolate()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a51f1d",
   "metadata": {},
   "source": [
    "4. `K-Nearest Neighbor (KNN) imputation`: This technique involves replacing missing values with the average value of K-nearest neighbors. It is useful when the data has complex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3800b6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  3.0\n",
      "1  2.0  4.0\n",
      "2  3.0  5.0\n",
      "3  4.0  6.0\n",
      "4  5.0  7.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# create a sample dataframe\n",
    "df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [3, 4, None, 6, 7]})\n",
    "\n",
    "# create KNN imputer object\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "# impute missing values\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(df_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17400245",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc4a32",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of classes or categories in the dataset is not equal. In other words, one class has significantly more observations than the other class or classes. For example, a dataset may have 95% of its observations belonging to one class and only 5% of observations belonging to the other class.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to several issues:\n",
    "\n",
    "`Biased model`: A machine learning model trained on imbalanced data may become biased towards the majority class. This is because the model tends to optimize the accuracy for the majority class, which leads to poor performance on the minority class.\n",
    "\n",
    "`Poor generalization`: Imbalanced data can lead to poor generalization of the model. This is because the model may learn patterns that are specific to the training data and not applicable to the unseen data.\n",
    "\n",
    "`Poor evaluation metrics`: Evaluation metrics such as accuracy may not be a good measure of performance for imbalanced data. For example, a model that always predicts the majority class will have a high accuracy but is not useful in practice.\n",
    "\n",
    "`Loss of information`: Imbalanced data can lead to a loss of information about the minority class. This is because the model may not have enough data to learn the patterns in the minority class.\n",
    "\n",
    "To handle imbalanced data, several techniques can be used such as oversampling the minority class, undersampling the majority class, using cost-sensitive learning, and using ensemble methods. These techniques can help improve the performance of the model on the minority class and prevent the issues caused by imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebbf633",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590977aa",
   "metadata": {},
   "source": [
    "`Upsampling` and `downsampling` are two techniques used in machine learning to handle imbalanced datasets. Both techniques are used to balance the class distribution in the dataset by adjusting the number of samples in each class.\n",
    "\n",
    "`Upsampling` refers to increasing the number of samples in the minority class to match the number of samples in the majority class. This can be done by duplicating the existing samples in the minority class or generating new samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "`Example`: Suppose we have a dataset with two classes - Class A and Class B. Class A has 100 observations, and Class B has only 20 observations. We can upsample Class B by duplicating the existing 20 observations or generating new samples using techniques such as SMOTE.\n",
    "\n",
    "`Downsampling` refers to decreasing the number of samples in the majority class to match the number of samples in the minority class. This can be done by randomly selecting a subset of the samples in the majority class or removing some samples from the majority class.\n",
    "\n",
    "`Example`: Suppose we have a dataset with two classes - Class A and Class B. Class A has 100 observations, and Class B has only 20 observations. We can downsample Class A by randomly selecting 20 observations from Class A and using them for training the model along with all observations in Class B.\n",
    "\n",
    "Up-sampling and down-sampling are required when the dataset is imbalanced, and the number of observations in one class is significantly larger than the number of observations in the other class. Imbalanced datasets can lead to biased models and poor performance on the minority class. To overcome this issue, up-sampling and down-sampling techniques can be used to balance the dataset and improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb72849e",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be022d7",
   "metadata": {},
   "source": [
    "`Data augmentation` is a technique used in machine learning to increase the amount of data in a dataset by creating new data points from the existing data points. This technique is used to prevent overfitting and improve the performance of the model.\n",
    "\n",
    "`SMOTE (Synthetic Minority Over-sampling Technique)` is a data augmentation technique used to handle imbalanced datasets. This technique generates synthetic samples for the minority class by interpolating between existing samples.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "For each sample in the minority class, SMOTE selects k nearest neighbors from the same class. The value of k is a hyperparameter that can be tuned based on the dataset.\n",
    "\n",
    "`SMOTE` then generates new samples by interpolating between the selected sample and its k nearest neighbors. For each new sample, SMOTE selects one of the k nearest neighbors randomly and interpolates between the selected sample and the neighbor.\n",
    "\n",
    "The `interpolation` is done by selecting a random point on the line segment connecting the selected sample and the neighbor. The coordinates of the new sample are then calculated based on this point.\n",
    "\n",
    "The process is repeated until the desired number of new samples is generated.\n",
    "\n",
    "`SMOTE` generates new samples that are similar to the existing samples but are not exact duplicates. This helps to prevent overfitting and improves the model's ability to generalize to new data.\n",
    "\n",
    "`SMOTE` can be applied to any machine learning algorithm and has been shown to be effective in handling imbalanced datasets. However, it should be used with caution as it can generate unrealistic samples in some cases. It is also important to ensure that the synthetic samples do not overlap with the existing samples in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c6e3e",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac9fddb",
   "metadata": {},
   "source": [
    "`Outliers` are data points that deviate significantly from the other data points in a dataset. They are data points that are distant from the bulk of the data, and they can occur due to various reasons such as measurement errors, data entry errors, or natural variability in the data.\n",
    "\n",
    "Handling outliers is essential for several reasons:\n",
    "\n",
    "1. They can significantly affect the statistical properties of the dataset: Outliers can distort the mean, variance, and other statistical measures of a dataset, leading to incorrect inferences and conclusions.\n",
    "\n",
    "2. They can lead to biased models: Outliers can significantly affect the training of machine learning models, leading to biased models that perform poorly on new data.\n",
    "\n",
    "3. They can affect the performance of some algorithms: Some machine learning algorithms, such as k-nearest neighbors, are sensitive to outliers and can be severely affected by their presence.\n",
    "\n",
    "4. They can cause problems in data visualization: Outliers can cause problems in visualizing the data, as they can cause the scaling of the axes to be distorted.\n",
    "\n",
    "To handle outliers, several techniques can be used such as removing the outliers, replacing them with other values such as the median, or transforming the data to reduce the impact of the outliers. The choice of technique depends on the nature of the data and the problem at hand.\n",
    "\n",
    "In summary, handling outliers is important because they can significantly affect the statistical properties of the data, lead to biased models, affect the performance of some algorithms, and cause problems in data visualization.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fdf6e",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e5c5f",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in a dataset. Here are some commonly used techniques:\n",
    "\n",
    "`Deleting rows with missing data`: This technique involves removing the entire row that contains missing data. However, this technique can result in a loss of information if there are a large number of missing values.\n",
    "\n",
    "`Deleting columns with missing data`: This technique involves removing the entire column that contains missing data. However, this technique can also result in a loss of information if there are important variables with missing values.\n",
    "\n",
    "`Imputation`: This technique involves filling in the missing values with estimated values. There are several ways to perform imputation, such as mean imputation, median imputation, mode imputation, and regression imputation.\n",
    "\n",
    "`Using algorithms that handle missing data`: Some machine learning algorithms, such as decision trees and random forests, can handle missing data by creating surrogate splits that take into account the missing data.\n",
    "\n",
    "`Using specialized techniques`: There are specialized techniques such as K-nearest neighbors imputation, hot-deck imputation, and expectation maximization that can be used to handle missing data.\n",
    "\n",
    "The choice of technique depends on the nature of the data and the problem at hand. For example, if there are only a few missing values, imputation may be a good choice. However, if there are many missing values, deleting rows or columns may be a better option. If the missing data is not handled properly, it can lead to biased analysis and inaccurate conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34db0b17",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5010e1e",
   "metadata": {},
   "source": [
    "To determine if the missing data is missing at random or if there is a pattern to the missing data, there are several strategies that can be used. Here are some commonly used strategies:\n",
    "\n",
    "`Visual inspection`: One strategy is to visually inspect the dataset for patterns or trends that may be related to the missing data. This can be done using scatterplots, histograms, and other visualization techniques.\n",
    "\n",
    "`Correlation analysis`: Another strategy is to use correlation analysis to determine if there is a relationship between the missing data and other variables in the dataset. This can be done using correlation matrices, scatterplots, and other statistical techniques.\n",
    "\n",
    "`Hypothesis testing`: Hypothesis testing can be used to determine if there is a significant difference between the missing data and other variables in the dataset. This can be done using t-tests, ANOVA, and other statistical tests.\n",
    "\n",
    "`Machine learning algorithms`: Machine learning algorithms can be used to determine if the missing data is related to other variables in the dataset. For example, a decision tree algorithm can be used to identify the variables that are most important in predicting missing data.\n",
    "\n",
    "`Missingness tests`: There are several statistical tests, such as Little's MCAR test, that can be used to determine if the missing data is missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR).\n",
    "\n",
    "The choice of strategy depends on the nature of the data and the problem at hand. If the missing data is found to be not missing at random (MNAR), it is important to handle it properly to avoid biased analysis and inaccurate conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271447e3",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8576e4",
   "metadata": {},
   "source": [
    "When working with imbalanced datasets, it is important to use evaluation metrics that take into account the class distribution. Here are some strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "`Confusion matrix`: The confusion matrix can be used to visualize the performance of the model. It shows the number of true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "`Precision and recall`: Precision and recall are important metrics for imbalanced datasets. Precision is the fraction of true positive predictions among all positive predictions, while recall is the fraction of true positives among all actual positives.\n",
    "\n",
    "`F1 score`: The F1 score is a measure of the harmonic mean of precision and recall. It is a good metric for imbalanced datasets because it gives equal weight to precision and recall.\n",
    "\n",
    "`Receiver Operating Characteristic (ROC) curve`: The ROC curve shows the tradeoff between true positive rate (TPR) and false positive rate (FPR) for different classification thresholds. A good model will have an ROC curve that is close to the upper left corner, which indicates high TPR and low FPR.\n",
    "\n",
    "`Area under the curve (AUC)`: The AUC is a single number that summarizes the ROC curve. It is a good metric for imbalanced datasets because it takes into account the tradeoff between TPR and FPR.\n",
    "\n",
    "`Resampling techniques`: Resampling techniques such as oversampling and undersampling can be used to balance the class distribution in the dataset and improve the performance of the model on the minority class.\n",
    "\n",
    "The choice of strategy depends on the nature of the data and the problem at hand. It is important to evaluate the performance of the model on both the majority and minority classes to ensure that it is effective in identifying the condition of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee950cce",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34efe0",
   "metadata": {},
   "source": [
    "When working with an unbalanced dataset, there are several methods that can be used to balance the dataset and down-sample the majority class. Here are some techniques:\n",
    "\n",
    "` Undersampling` : involves randomly selecting a subset of the majority class to balance the class distribution. This can be done using techniques such as random undersampling or Tomek links.\n",
    "\n",
    "`Oversampling` : Oversampling involves generating synthetic data points for the minority class to balance the class distribution. This can be done using techniques such as SMOTE or ADASYN.\n",
    "\n",
    "`Combination of undersampling and oversampling`: A combination of undersampling and oversampling can be used to balance the class distribution. This can be done using techniques such as SMOTE combined with Tomek links.\n",
    "\n",
    "`Weighted models`: Weighted models can be used to give more weight to the minority class in the training process. This can be done using techniques such as class weighting or cost-sensitive learning.\n",
    "\n",
    "To down-sample the majority class, undersampling techniques such as random undersampling or Tomek links can be used. Random undersampling involves randomly selecting a subset of the majority class to balance the class distribution, while Tomek links involves removing the overlapping data points between the majority and minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b922cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    8961\n",
       "1    1039\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X,y= make_classification(n_samples=10000, n_redundant=0, n_features=2, \n",
    "                    n_clusters_per_class=1, weights=[0.90], random_state=12\n",
    "                   )\n",
    "df1 = pd.DataFrame(X, columns=['f1','f2'])\n",
    "df2 = pd.DataFrame(y, columns=['target'])\n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99354cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1039\n",
       "1    1039\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.target==0]\n",
    "df_minority = df[df.target==1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=len(df_minority),    # to match minority class\n",
    "                                 random_state=42)  # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled.target.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6207d90",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb3a5b",
   "metadata": {},
   "source": [
    "When working with a dataset that has a low percentage of occurrences of a rare event, there are several methods that can be used to balance the dataset and up-sample the minority class. Here are some techniques:\n",
    "\n",
    "`Oversampling`: Oversampling involves generating synthetic data points for the minority class to balance the class distribution. This can be done using techniques such as SMOTE or ADASYN.\n",
    "\n",
    "`Synthetic Minority Over-sampling Technique (SMOTE)`: SMOTE is a popular oversampling technique that generates synthetic data points for the minority class by interpolating between existing data points. It works by selecting a data point from the minority class and finding its k-nearest neighbors. Synthetic data points are then generated by interpolating between the selected data point and its k-nearest neighbors.\n",
    "\n",
    "`Adaptive Synthetic (ADASYN)`: ADASYN is a variation of SMOTE that generates more synthetic data points for data points that are harder to classify, i.e., those data points that are closer to the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1414a6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
